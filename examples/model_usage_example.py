#!/usr/bin/env python3
"""
Ejemplo de uso de la capa de abstracci√≥n de modelos.

Este script demuestra c√≥mo usar los diferentes backends
(Ollama, OpenAI Local, HuggingFace) de forma transparente.

Requisitos:
- Ollama corriendo en localhost:11434
- Modelo llama3.2:1b descargado (ollama pull llama3.2:1b)

Ejecutar:
    python examples/model_usage_example.py
"""

import asyncio
import sys
from pathlib import Path

# A√±adir el directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models import ModelFactory, get_model, get_model_manager


async def basic_generation_example():
    """Ejemplo b√°sico de generaci√≥n de texto."""
    print("\n" + "=" * 60)
    print("EJEMPLO 1: Generaci√≥n B√°sica con Ollama")
    print("=" * 60)
    
    try:
        # Crear adaptador usando la factory
        model = ModelFactory.create("ollama/llama3.2:1b")
        
        # Verificar conexi√≥n
        is_healthy = await model.health_check()
        if not is_healthy:
            print("‚ùå Ollama no est√° disponible. Aseg√∫rate de que est√° corriendo.")
            return
        
        print("‚úÖ Conexi√≥n con Ollama establecida")
        
        # Generar respuesta
        messages = [
            {"role": "system", "content": "Eres un asistente conciso. Responde en una oraci√≥n."},
            {"role": "user", "content": "¬øCu√°l es la capital de Espa√±a?"}
        ]
        
        print("\nüìù Prompt: ¬øCu√°l es la capital de Espa√±a?")
        print("‚è≥ Generando respuesta...")
        
        response = await model.generate(
            messages,
            temperature=0.7,
            max_tokens=100,
        )
        
        print(f"\nüí¨ Respuesta: {response.content}")
        print(f"\nüìä M√©tricas:")
        print(f"   - Modelo: {response.model}")
        print(f"   - Tokens prompt: {response.prompt_tokens}")
        print(f"   - Tokens generados: {response.completion_tokens}")
        print(f"   - Tiempo: {response.generation_time_ms:.0f}ms")
        if response.tokens_per_second:
            print(f"   - Velocidad: {response.tokens_per_second:.1f} tok/s")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def streaming_example():
    """Ejemplo de generaci√≥n con streaming."""
    print("\n" + "=" * 60)
    print("EJEMPLO 2: Generaci√≥n con Streaming")
    print("=" * 60)
    
    try:
        model = ModelFactory.create("ollama/llama3.2:1b")
        
        is_healthy = await model.health_check()
        if not is_healthy:
            print("‚ùå Ollama no est√° disponible")
            return
        
        messages = [
            {"role": "user", "content": "Cuenta del 1 al 5 en espa√±ol, un n√∫mero por l√≠nea."}
        ]
        
        print("\nüìù Prompt: Cuenta del 1 al 5")
        print("‚è≥ Streaming respuesta:\n")
        
        print("üí¨ ", end="")
        async for chunk in model.generate_stream(messages, max_tokens=50):
            print(chunk, end="", flush=True)
        print("\n")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def model_manager_example():
    """Ejemplo usando el ModelManager para gestionar m√∫ltiples modelos."""
    print("\n" + "=" * 60)
    print("EJEMPLO 3: Usando ModelManager")
    print("=" * 60)
    
    try:
        manager = get_model_manager()
        
        # Precargar modelos
        print("\n‚è≥ Precargando modelos...")
        results = await manager.preload(
            ["ollama/llama3.2:1b"],
            verify=True,
        )
        
        for model_id, success in results.items():
            status = "‚úÖ" if success else "‚ùå"
            print(f"   {status} {model_id}")
        
        # Listar modelos cargados
        loaded = manager.list_loaded()
        print(f"\nüìã Modelos cargados: {loaded}")
        
        # Health check de todos
        print("\nüè• Health check de todos los modelos:")
        health = await manager.health_check_all()
        for model_id, healthy in health.items():
            status = "‚úÖ Healthy" if healthy else "‚ùå Unhealthy"
            print(f"   {model_id}: {status}")
        
        # Usar un modelo
        if "ollama/llama3.2:1b" in manager:
            model = await manager.get("ollama/llama3.2:1b")
            response = await model.generate([
                {"role": "user", "content": "Di 'Hola' en tres idiomas"}
            ], max_tokens=50)
            print(f"\nüí¨ Respuesta: {response.content}")
        
        # Limpiar
        await manager.cleanup()
        print("\nüßπ Modelos descargados")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def model_info_example():
    """Ejemplo de obtenci√≥n de informaci√≥n del modelo."""
    print("\n" + "=" * 60)
    print("EJEMPLO 4: Informaci√≥n del Modelo")
    print("=" * 60)
    
    try:
        model = ModelFactory.create("ollama/llama3.2:1b")
        
        is_healthy = await model.health_check()
        if not is_healthy:
            print("‚ùå Ollama no est√° disponible")
            return
        
        # Obtener informaci√≥n del modelo
        info = await model.get_model_info()
        
        print("\nüìÑ Informaci√≥n del modelo:")
        for key, value in info.items():
            print(f"   {key}: {value}")
        
        # Listar modelos disponibles
        models = await model.list_models()
        print(f"\nüìã Modelos disponibles en Ollama ({len(models)}):")
        for m in models[:5]:  # Mostrar solo los primeros 5
            print(f"   - {m.get('name')}")
        if len(models) > 5:
            print(f"   ... y {len(models) - 5} m√°s")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def embedding_example():
    """Ejemplo de generaci√≥n de embeddings."""
    print("\n" + "=" * 60)
    print("EJEMPLO 5: Generaci√≥n de Embeddings")
    print("=" * 60)
    
    try:
        model = ModelFactory.create("ollama/nomic-embed-text")
        
        is_healthy = await model.health_check()
        if not is_healthy:
            print("‚ùå Modelo de embeddings no disponible")
            print("   Ejecuta: ollama pull nomic-embed-text")
            return
        
        texts = [
            "La inteligencia artificial est√° transformando la educaci√≥n",
            "Los robots aprenden como los humanos",
            "Me gusta el helado de chocolate",
        ]
        
        print("\nüìù Textos a embeber:")
        for i, text in enumerate(texts, 1):
            print(f"   {i}. {text}")
        
        print("\n‚è≥ Generando embeddings...")
        
        response = await model.embed(texts)
        
        print(f"\nüìä Resultados:")
        print(f"   - Dimensiones: {response.dimensions}")
        print(f"   - N√∫mero de vectores: {len(response.embeddings)}")
        print(f"   - Tiempo: {response.generation_time_ms:.0f}ms")
        
        # Mostrar primeros valores de cada embedding
        print("\nüî¢ Primeros 5 valores de cada embedding:")
        for i, emb in enumerate(response.embeddings, 1):
            preview = ", ".join(f"{v:.4f}" for v in emb[:5])
            print(f"   {i}. [{preview}, ...]")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def main():
    """Ejecuta todos los ejemplos."""
    print("üéì Aula AI Tutor - Ejemplos de Capa de Modelos")
    print("=" * 60)
    
    await basic_generation_example()
    await streaming_example()
    await model_manager_example()
    await model_info_example()
    await embedding_example()
    
    print("\n" + "=" * 60)
    print("‚úÖ Ejemplos completados")
    print("=" * 60)
    
    # Limpiar cach√©
    ModelFactory.clear_cache()


if __name__ == "__main__":
    asyncio.run(main())
